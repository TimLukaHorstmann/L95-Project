{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences to Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"My aunt's can opener can open a drum.\", 'The old car broke down in the car park.', 'At least two men broke in and stole my TV.', 'Kim and Sandy both broke up with their partners.', 'The horse as well as the rabbits which we wanted to eat has escaped.', \"It was my aunt's car which we sold at auction last year in February.\", 'Natural disasters – storms, flooding, hurricanes – occur infrequently but cause devastation that strains resources to breaking point.', 'Letters delivered on time by old-fashioned means are increasingly rare, so it is as well that that is not the only option available.', \"English also has many words of more or less unique function, including interjections (oh, ah), negatives (no, not), politeness markers (please, thank you), and the existential 'there' (there are horses but not unicorns) among others.\", 'The Penn Treebank tagset was culled from the original 87-tag tagset for the Brown Corpus.', 'For example the original Brown and C5 tagsets include a separate tag for each of the different forms of the verbs do (e.g. C5 tag VDD for did and VDG tag for doing), be and have.']\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences_from_gold_standard(file_path):\n",
    "    original_sentences = []\n",
    "    gold_trees = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            # Check if the line starts with a number followed by a colon, which indicates the start of a sentence block\n",
    "            if line.strip() and line.split(':', 1)[0].strip().replace('a', '').replace('b', '').isdigit():\n",
    "                sentence = line.split(':', 1)[1].strip()\n",
    "                original_sentences.append(sentence)\n",
    "    return original_sentences\n",
    "\n",
    "# Specify the path to your gold standard file\n",
    "file_path = 'L95_10sentencesTaggedAndParsed_goldStandard_corrected.txt'\n",
    "\n",
    "# Extract the sentences\n",
    "extracted_sentences = extract_sentences_from_gold_standard(file_path)\n",
    "print(extracted_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_gold_standard(file_path):\n",
    "    original_sentences = []\n",
    "    gold_trees = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line and line.split(':', 1)[0].strip().replace('a', '').replace('b', '').isdigit():\n",
    "                sentence = line.split(':', 1)[1].strip()\n",
    "                original_sentences.append(sentence)\n",
    "                # The gold tree notation is on the third line after the sentence line\n",
    "                gold_tree = lines[i+2].strip()\n",
    "                gold_trees.append(gold_tree)\n",
    "                i += 3  # Move to the next block of sentence\n",
    "            else:\n",
    "                i += 1  # Move to the next line\n",
    "    return original_sentences, gold_trees\n",
    "\n",
    "# Specify the path to your gold standard file\n",
    "file_path = 'L95_10sentencesTaggedAndParsed_goldStandard_corrected.txt'\n",
    "\n",
    "# Extract the sentences and gold trees\n",
    "extracted_sentences, gold_trees = extract_sentences_from_gold_standard(file_path)\n",
    "print(extracted_sentences)\n",
    "print(gold_trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Charniak-Johnson parser or Brown Reranking Parser\n",
    "https://aclanthology.org/P05-1022.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parser is already loaded and can only be loaded once.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbllipparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RerankingParser\n\u001b[0;32m----> 2\u001b[0m rrp \u001b[38;5;241m=\u001b[39m \u001b[43mRerankingParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_and_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWSJ+Gigaword-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/tihorstm/L95/Project_Code/.venv/lib/python3.8/site-packages/bllipparser/RerankingParser.py:905\u001b[0m, in \u001b[0;36mRerankingParser.fetch_and_load\u001b[0;34m(this_class, model_name, models_directory, verbose, extra_loading_options)\u001b[0m\n\u001b[1;32m    900\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m download_and_install_model(model_name,\n\u001b[1;32m    901\u001b[0m                                        models_directory,\n\u001b[1;32m    902\u001b[0m                                        verbose)\n\u001b[1;32m    904\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m extra_loading_options \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthis_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_unified_model_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/tihorstm/L95/Project_Code/.venv/lib/python3.8/site-packages/bllipparser/RerankingParser.py:876\u001b[0m, in \u001b[0;36mRerankingParser.from_unified_model_dir\u001b[0;34m(this_class, model_dir, parsing_options, reranker_options, parser_only)\u001b[0m\n\u001b[1;32m    874\u001b[0m rrp \u001b[38;5;241m=\u001b[39m this_class()\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser_model_dir:\n\u001b[0;32m--> 876\u001b[0m     \u001b[43mrrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_parser_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparsing_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reranker_features_filename \u001b[38;5;129;01mand\u001b[39;00m reranker_weights_filename \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    878\u001b[0m    \u001b[38;5;129;01mnot\u001b[39;00m parser_only:\n\u001b[1;32m    879\u001b[0m     rrp\u001b[38;5;241m.\u001b[39mload_reranker_model(reranker_features_filename,\n\u001b[1;32m    880\u001b[0m                             reranker_weights_filename,\n\u001b[1;32m    881\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreranker_options)\n",
      "File \u001b[0;32m/home/tihorstm/L95/Project_Code/.venv/lib/python3.8/site-packages/bllipparser/RerankingParser.py:581\u001b[0m, in \u001b[0;36mRerankingParser.load_parser_model\u001b[0;34m(self, model_dir, terms_only, heads_only, **parser_options)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the parsing model from model_dir and set parsing\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03moptions. In general, the default options should suffice but see\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mthe set_parser_options() method for details. Note that the parser\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03mTree.dependencies(). If both are set to True, both of these will\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03mbe loaded but the full parsing model will not.\"\"\"\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RerankingParser\u001b[38;5;241m.\u001b[39m_parser_model_loaded:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParser is already loaded and can only \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    582\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbe loaded once.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_path_or_error(model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParser model directory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (terms_only \u001b[38;5;129;01mor\u001b[39;00m heads_only):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parser is already loaded and can only be loaded once."
     ]
    }
   ],
   "source": [
    "from bllipparser import RerankingParser\n",
    "rrp = RerankingParser.fetch_and_load('WSJ+Gigaword-v2', verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrp.simple_parse(\"It's that easy.\")\n",
    "# as a tagger:\n",
    "# rrp.tag(\"Time flies while you're having fun.\")\n",
    "\n",
    "charniak_trees = [rrp.simple_parse(sentence) for sentence in extracted_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berkeley Neural Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "\n",
    "benepar.download('benepar_en3')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "        nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tihorstm/L95/Project_Code/.venv/lib/python3.8/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "berkeley_trees = []\n",
    "for sentence in extracted_sentences:\n",
    "    doc = nlp(sentence)\n",
    "    sent = list(doc.sents)[0]\n",
    "    parse_tree = sent._.parse_string\n",
    "    berkeley_trees.append(parse_tree)\n",
    "\n",
    "# print(berkeley_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import *\n",
    "from IPython.display import display, Image, SVG\n",
    "\n",
    "def printTree(parsed_tree: str, parser_name: str, index: int, saveFile = True):\n",
    "    nltk_tree = Tree.fromstring(parsed_tree) # convert into NLTK tree\n",
    "\n",
    "    # nltk_tree.pformat_latex_qtree() # get latex code representation of the tree compatible with the LaTeX qtree package\n",
    "\n",
    "    # print to console\n",
    "    # tree.pretty_print(unicodelines=True, nodedist=4) # print to console\n",
    "\n",
    "    # print as SVG\n",
    "    svg_format = nltk_tree._repr_svg_()\n",
    "    # display(SVG(svg_format))\n",
    "\n",
    "    if saveFile:\n",
    "        with open(f\"ParseTrees/{parser_name}_{index}.svg\", 'w') as f:  # open a file in write mode\n",
    "            f.write(svg_format)  # write the SVG content to the file\n",
    "    \n",
    "    return nltk_tree\n",
    "\n",
    "def tree_to_forest(tree):\n",
    "    \"\"\"\n",
    "    Convert an NLTK Tree object's string representation into a LaTeX forest package representation.\n",
    "    Escapes special LaTeX characters.\n",
    "    \"\"\"\n",
    "    # Base case: if the tree is a leaf, just return the escaped leaf\n",
    "    if isinstance(tree, str):\n",
    "        return f\"[{tree}]\"\n",
    "    \n",
    "    # Recursively convert each subtree\n",
    "    escaped_label = tree.label()\n",
    "    result = '[' + ' '.join([escaped_label] + [tree_to_forest(t) for t in tree]) + ']'\n",
    "    return result\n",
    "\n",
    "def toLatexFigures(latex_trees, parser_names, indices):\n",
    "    with open(\"parse_trees.tex\", 'w') as f:\n",
    "        for latex_tree, parser_name, index in zip(latex_trees, parser_names, indices):\n",
    "            f.write('\\\\thispagestyle{empty}\\n')\n",
    "            f.write('\\\\begin{center}\\n')\n",
    "            f.write(f'{{\\\\Large \\\\textbf{{Model: {parser_name} - Sentence {index+1}}}}}\\n\\n')  # Header\n",
    "            f.write('\\\\vspace*{\\\\fill}\\n')  # Center the tree vertically\n",
    "            f.write('\\\\begin{forest}\\n')\n",
    "            f.write(latex_tree + '\\n')\n",
    "            f.write('\\\\end{forest}\\n')\n",
    "            f.write('\\\\vspace*{\\\\fill}\\n')  # Center the tree vertically\n",
    "            f.write('\\\\end{center}\\n')\n",
    "            f.write('\\\\newpage\\n\\n')  # Ensures a new page for the next tree\n",
    "\n",
    "latex_trees = []\n",
    "parser_names = []\n",
    "indices = []\n",
    "\n",
    "for index, (charniak_tree, berkeley_tree) in enumerate(zip(charniak_trees, berkeley_trees)):\n",
    "    charniak_tree_nltk = printTree(charniak_tree, \"Charniak\", index, True)\n",
    "    berkeley_tree_nltk = printTree(berkeley_tree, \"Berkeley\", index, True)\n",
    "\n",
    "    # Convert the NLTK tree objects to forest package strings\n",
    "    charniak_forest = tree_to_forest(charniak_tree_nltk)\n",
    "    berkeley_forest = tree_to_forest(berkeley_tree_nltk)\n",
    "\n",
    "    # Append the LaTeX forest representations to the list\n",
    "    latex_trees.append(charniak_forest)\n",
    "    latex_trees.append(berkeley_forest)\n",
    "\n",
    "    parser_names.extend([\"Charniak\", \"Berkeley\"])\n",
    "    indices.extend([index, index])\n",
    "\n",
    "toLatexFigures(latex_trees, parser_names, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(S1 (S (NP (NP (PRP$ My) (NN aunt) (POS 's)) (NN can) (NN opener)) (VP (MD can) (VP (VB open) (NP (DT a) (VB drum)))) (. .)))\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charniak_trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
