{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences to Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"my aunt's can opener can open a drum.\", 'the old car broke down in the car park.', 'at least two men broke in and stole my tv.', 'kim and sandy both broke up with their partners.', 'the horse as well as the rabbits which we wanted to eat has escaped.', \"it was my aunt's car which we sold at auction last year in february.\", 'natural disasters – storms, flooding, hurricanes – occur infrequently but cause devastation that strains resources to breaking point.', 'letters delivered on time by old-fashioned means are increasingly rare, so it is as well that that is not the only option available.', \"english also has many words of more or less unique function, including interjections (oh, ah), negatives (no, not), politeness markers (please, thank you), and the existential 'there' (there are horses but not unicorns) among others.\", 'the penn treebank tagset was culled from the original 87-tag tagset for the brown corpus.', 'for example the original brown and c5 tagsets include a separate tag for each of the different forms of the verbs do (e.g. c5 tag vdd for did and vdg tag for doing), be and have.']\n",
      "[\"(TOP  (S   (NP (PRP$ my:0) (NP (NN aunt:1) (NP (POS 's+:2) (NP (NN can:3) (NN opener:4)))))   (VP (MD can:5) (VP (VB open:6) (NP (DT a:7) (NN drum:8)))))  (. .:9))\", '(TOP  (S (NP (DT the:0) (NP (ADJP (JJ old:1)) (NN car:2)))   (VP (V (VBD break+ed:3) (RP down:4))    (PP (IN in:5) (NP (DT the:6) (NP (NN car:7) (NN park:8))))))  (. .:9))', '(TOP  (S (NP (ADVP (RB at:0)   (ADVP (RBS least:1) (CD two:2))) (NNS man+s:3))     (VP (VP (V (VBD break+ed:4) (RP in:5)))      (CONJP (CC and:6)       (VP (VBD steal+ed:7) (NP (PRP$ my:8) (NN tv:9))))))  (. .:10))', '(TOP (S (NP (NP (NNP Kim:0)) (CONJP (CC and:1) (NP (NNP Sandy:2)))) (VP (ADVP (RB both:3))   (VP (V (VBD break+ed:4) (RP up:5))    (PP (IN with:6) (NP (PRP$ their:7) (NNS partner+s:8))))))  (. .:9))', '(TOP  (S   (NP (NP (DT the:0) (NN horse:1))    (CONJP (ADVP (RB as:2) (ADVP (RB well:3)) (ADVP (RB as:4)))       (NP (NP (DT the:5) (NNS rabbit+s:6))        (S (WDT which:7)         (S (PRP we:8)          (VP (VBD want+ed:9) (VP (TO to:10) (VB eat:11))))))))   (VP (VBZ have+s:12) (VP (VBN escape+ed:13))))  (. .:14))', \"(TOP  (S (NP (PRP it:0))   (VP (VBD be+ed:1)    (NP (NP (PRP$ my:2) (NP (NN aunt:3) (NP (POS 's+:4) (NN car:5))))     (S (WDT which:6)      (S (NP (PRP we:7))       (VP (VP (VP (VBD sell+ed:8) (PP (IN at:9) (NP (NN auction:10))))        (ADVP (RB last:11) (NP (NN year:12))))         (PP (IN in:13) (NP (NNP February:14)))))))))  (. .:15))\", '(TOP (S (NP (NP (ADJP (JJ natural:0)) (NNS disaster+s:1)) (PRN (PRN (: –:2) (NP (NP (NP (NNS storm+s:3)) (LST (, ,:4) (NP (NN flood+ing:5)))) (LST (, ,:6)     (NP (NNS hurricane+s:7))))) (: –:8))) (VP (VP (VP (VBP occur:9)) (ADVP (RB infrequently:10)))  (CONJP (CC but:11) (VP (VBP cause:12) (NP (NN devastation:13) (S (WDT that:14)   (VP (VP (VBZ strain+s:15)    (NP (NNS resource+s:16)))     (PP (IN to:17) (NP (NN break+ing:18) (NN point:19))))))))))  (. .:20))', '(TOP  (S (NP (NP (NNS letter+s:0))   (S (VP (VP (VBN deliver+ed:1) (PP (IN on:2) (NP (NN time:3))))    (PP (IN by:4) (NP (ADJP (ADJP (ADJP (JJ old:5))  (HYPH -:6))  (JJ fashion+ed:7)) (NNS means:8))))))    (VP (VBP be+:9) (ADJP (ADVP (RB increasingly:10)) (JJ rare:11)))   (S (ADVP (, ,:12) (RB so:13))    (S (NP (PRP it:14))     (VP (VP (VBZ be+s:15)      (ADVP (RB as:16)       (ADVP (RB well:17))))        (S (IN that:18)         (S (NP (DT that:19))          (VP (VBZ be+s:20) (ADVP (RB not+:21)           (NP (NP (DT the:22) (NP (ADJP (JJ only:23)) (NN option:24)))            (ADJP (JJ available:25)))))))))))  (. .:26))', \"(TOP (S (NP (NNP english:0))  (VP (ADVP (RB also:1))   (VP (VBZ have+s:2)    (NP (NP (ADJP (JJ many:3)) (NP (NNS word+s:4) (PP (IN of:5) (NP (NP (ADJP (ADJP (JJR more:6))     (CONJP (CC or:7) (ADJP (JJR less:8)))) (NP (ADJP (JJ unique:9)) (NN function:10))) (VP (, ,:11)     (VP (VP (VP (VP (VBG include+ing:12) (NP (NP (NNS interjection+s:13)) (PRN (-LRB- \\\\(:14)      (LST (LST (UH oh:15) (, ,:16)) (UH ah:17)) (-RRB- \\\\):18)))     (, ,:19) (NP (NP (NNS negative+s:20))  (PRN (-LRB- \\\\(:21) (LST (LST (RB no:22) (, ,:23)) (RB not+:24)) (-RRB- \\\\):25))) (, ,:26)  (NP (NP (NN politeness:27) (NNS marker+s:28)) (PRN (-LRB- \\\\(:29)  (LST (LST (UH please:30) (, ,:31)) (VP (VB thank:32) (NP (PRP you:33))))  (-RRB- \\\\):34))) (, ,:35)  (CONJP (CC and:36) (ADVP (ADVP (ADVP (DT the:37) (ADVP (ADJP (JJ existential:38)) (`` ':39)  (ADVP (RB there:40)))) ('' ':41)) (PRN (-LRB- \\\\(:42)  (S (EX there:43)   (VP (VBP be+:44)    (NP (NNS horse+s:45) (CONJP (CC but:46) (ADVP (RB not+:47))     (NP (NNS unicorn+s:48)))))))))  (-RRB- \\\\):49))) (PP (IN among:50) (NP (NNS other+s:51)))))))))))))) (. .:52))\", '(TOP  (S (NP (DT the:0) (NP (NNP Penn:1) (NP (NNP Treebank:2) (NP (NN tagset:3)))))   (VP (VP (VBD be+ed:4) (VBN cull+ed:5))    (PP (IN from:6) (NP (DT the:7) (ADJP (JJ original:8) (NP (NP (CD 87:9)  (HYPH -:10))  (NP (NN tag:11)))) (NP (NN tagset:12)   (PP (IN for:13) (NP (DT the:14) (NP (NNP Brown:15) (NP (NNP corpus:16))))))))))  (. .:17))', '(TOP  (S (PP (IN for:0)   (NP (NN example:1)))    (S     (NP (DT the:2) (ADJP (JJ original:3) (NP (NNP Brown:4)) (CONJP (CC and:5)      (NP (NP (NNP c5:6)) (NNS tagset+s:7)))))     (VP (VBP include:8)      (NP (DT a:9) (ADJP (JJ separate:10) (NP (NN tag:11)       (PP (PP (IN for:12) (DT each:13))      (PP (IN of:14)       (NP (DT the:15) (ADJP (JJ different:16) (NP (NNS form+s:17)        (PP (IN of:18) (NP (DT the:19) (NP (NNS verb+s:20)  (LST (LST (LST (VB do:21) (PRN (-LRB- \\\\(:22)  (ADVP (ADVP (RB e.g.:23) (NP (NNP c5:24) (NP (NN tag:25) (NN Vdd:26)))  (PP (IN for:27) (VBN do+ed:28)))  (CONJP (CC and:29) (NP (NN Vdg:30) (NN tag:31))   (PP (IN for:32) (VBG doing:33))))  (-RRB- \\\\):34))) (, ,:35) (VB be:36)) (CONJP (CC and:37) (VB have:38))))))))))))))))) (. .:39))']\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences_from_gold_standard(file_path):\n",
    "    original_sentences = []\n",
    "    gold_trees = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line and line.split(':', 1)[0].strip().replace('a', '').replace('b', '').isdigit():\n",
    "                sentence = line.split(':', 1)[1].strip()\n",
    "                original_sentences.append(sentence.lower())\n",
    "                # The gold tree notation is on the third line after the sentence line\n",
    "                gold_tree = lines[i+2].strip()\n",
    "                gold_trees.append(gold_tree)\n",
    "                i += 3  # Move to the next block of sentence\n",
    "            else:\n",
    "                i += 1  # Move to the next line\n",
    "    return original_sentences, gold_trees\n",
    "\n",
    "# Specify the path to your gold standard file\n",
    "file_path = 'L95_10sentencesTaggedAndParsed_goldStandard_corrected.txt'\n",
    "\n",
    "# Extract the sentences and gold trees\n",
    "extracted_sentences, gold_trees = extract_sentences_from_gold_standard(file_path)\n",
    "print(extracted_sentences)\n",
    "print(gold_trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Charniak-Johnson parser or Brown Reranking Parser\n",
    "https://aclanthology.org/P05-1022.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bllipparser import RerankingParser\n",
    "rrp = RerankingParser.fetch_and_load('WSJ+Gigaword-v2', verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrp.simple_parse(\"It's that easy.\")\n",
    "# as a tagger:\n",
    "# rrp.tag(\"Time flies while you're having fun.\")\n",
    "\n",
    "charniak_trees = [rrp.simple_parse(sentence) for sentence in extracted_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berkeley Neural Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "/home/tihorstm/L95/Project_Code/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "\n",
    "benepar.download('benepar_en3')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "        nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tihorstm/L95/Project_Code/.venv/lib/python3.8/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "berkeley_trees = []\n",
    "for sentence in extracted_sentences:\n",
    "    doc = nlp(sentence)\n",
    "    sent = list(doc.sents)[0]\n",
    "    parse_tree = sent._.parse_string\n",
    "    berkeley_trees.append(parse_tree)\n",
    "\n",
    "# print(berkeley_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import *\n",
    "from IPython.display import display, Image, SVG\n",
    "from cairosvg import svg2png\n",
    "\n",
    "def printTree(parsed_tree: str, parser_name: str, index: int, saveFile = True):\n",
    "    nltk_tree = Tree.fromstring(parsed_tree) # convert into NLTK tree\n",
    "\n",
    "    # nltk_tree.pformat_latex_qtree() # get latex code representation of the tree compatible with the LaTeX qtree package\n",
    "\n",
    "    # print to console\n",
    "    # tree.pretty_print(unicodelines=True, nodedist=4) # print to console\n",
    "\n",
    "    # print as SVG\n",
    "    svg_format = nltk_tree._repr_svg_()\n",
    "    # display(SVG(svg_format))\n",
    "\n",
    "    if saveFile:\n",
    "        with open(f\"ParseTrees/{parser_name}_{index}.svg\", 'w') as f:  # open a file in write mode\n",
    "            f.write(svg_format)  # write the SVG content to the file\n",
    "        svg2png(bytestring=svg_format,write_to=f\"ParseTrees/{parser_name}_{index}.png\")\n",
    "    \n",
    "    return nltk_tree\n",
    "\n",
    "def tree_to_forest(tree):\n",
    "    \"\"\"\n",
    "    Convert an NLTK Tree object's string representation into a LaTeX forest package representation.\n",
    "    Escapes special LaTeX characters.\n",
    "    \"\"\"\n",
    "    # Base case: if the tree is a leaf, just return the escaped leaf\n",
    "    if isinstance(tree, str):\n",
    "        return f\"[{tree}]\"\n",
    "    \n",
    "    # Recursively convert each subtree\n",
    "    escaped_label = tree.label()\n",
    "    result = '[' + ' '.join([escaped_label] + [tree_to_forest(t) for t in tree]) + ']'\n",
    "    return result\n",
    "\n",
    "def toLatexFigures(latex_trees, parser_names, indices):\n",
    "    with open(\"parse_trees.tex\", 'w') as f:\n",
    "        for latex_tree, parser_name, index in zip(latex_trees, parser_names, indices):\n",
    "            f.write('\\\\thispagestyle{empty}\\n')\n",
    "            f.write('\\\\begin{center}\\n')\n",
    "            f.write(f'{{\\\\Large \\\\textbf{{Model: {parser_name} - Sentence {index+1}}}}}\\n\\n')  # Header\n",
    "            f.write('\\\\vspace*{\\\\fill}\\n')  # Center the tree vertically\n",
    "            f.write('\\\\begin{forest}\\n')\n",
    "            f.write(latex_tree + '\\n')\n",
    "            f.write('\\\\end{forest}\\n')\n",
    "            f.write('\\\\vspace*{\\\\fill}\\n')  # Center the tree vertically\n",
    "            f.write('\\\\end{center}\\n')\n",
    "            f.write('\\\\newpage\\n\\n')  # Ensures a new page for the next tree\n",
    "\n",
    "latex_trees = []\n",
    "parser_names = []\n",
    "indices = []\n",
    "\n",
    "for index, (charniak_tree, berkeley_tree, gold_tree) in enumerate(zip(charniak_trees, berkeley_trees, gold_trees)):\n",
    "    charniak_tree_nltk = printTree(charniak_tree, \"Charniak\", index, True)\n",
    "    berkeley_tree_nltk = printTree(berkeley_tree, \"Berkeley\", index, True)\n",
    "    gold_tree_nltk = printTree(gold_tree, \"Gold\", index, True)\n",
    "\n",
    "    # Convert the NLTK tree objects to forest package strings\n",
    "    charniak_forest = tree_to_forest(charniak_tree_nltk)\n",
    "    berkeley_forest = tree_to_forest(berkeley_tree_nltk)\n",
    "    gold_forest = tree_to_forest(gold_tree_nltk)\n",
    "\n",
    "    # Append the LaTeX forest representations to the list\n",
    "    latex_trees.append(charniak_forest)\n",
    "    latex_trees.append(berkeley_forest)\n",
    "    latex_trees.append(gold_forest)\n",
    "\n",
    "    parser_names.extend([\"Charniak\", \"Berkeley\", \"Gold\"])\n",
    "    indices.extend([index, index, index])\n",
    "\n",
    "toLatexFigures(latex_trees, parser_names, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation Using EVALB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust output to unify all three results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def adjust_gold_trees(trees):\n",
    "    adjusted_trees = []\n",
    "    for tree in trees:\n",
    "        # Remove the first \"(TOP \" and the last \")\"\n",
    "        tree = tree[5:-1].strip()\n",
    "\n",
    "        # Remove all \":<number>\" occurrences\n",
    "        tree = re.sub(r':\\d+', '', tree)\n",
    "\n",
    "        # Replace specific word forms\n",
    "        replacements = {\n",
    "            \"break+ed\": \"broke\",\n",
    "            \"man+s\": \"men\",\n",
    "            \"Kim\": \"kim\",\n",
    "            \"rabbit+s\": \"rabbits\",\n",
    "            \"be+ed\": \"was\",\n",
    "            \"disaster+s\": \"disasters\",\n",
    "            \"letter+s\": \"letters\",\n",
    "            \"steal+ed\": \"stole\",\n",
    "            \"Sandy\": \"sandy\",\n",
    "            \"want+ed\": \"wanted\",\n",
    "            \"sell+ed\": \"sold\",\n",
    "            \"storm+s\": \"storms\",\n",
    "            \"deliver+ed\": \"delivered\",\n",
    "            \"have+s\": \"has\",\n",
    "            \"Penn\": \"penn\",\n",
    "            \"Brown\": \"brown\",\n",
    "            \"partner+s\":\"partners\",\n",
    "            \"escape+ed\":\"escaped\",\n",
    "            \"February\":\"february\",\n",
    "            \"flood+ing\":\"flooding\",\n",
    "            \"fashion+ed\":\"fashioned\",\n",
    "            \"word+s\":\"words\",\n",
    "            \"Treebank\":\"treebank\",\n",
    "            \"tagset+s\":\"tagsets\",\n",
    "            \"hurricane+s\":\"hurricanes\",\n",
    "            \"be+s\":\"is\",\n",
    "            \"be+\":\"are\",\n",
    "            \"include+ing\":\"including\",\n",
    "            \"cull+ed\":\"culled\",\n",
    "            \"form+s\":\"forms\",\n",
    "            \"strain+s\":\"strains\",\n",
    "            \"interjection+s\":\"interjections\",\n",
    "            \"verb+s\":\"verbs\",\n",
    "            \"resource+s\":\"resources\",\n",
    "            \"not+\":\"not\",\n",
    "            \"negative+s\":\"negatives\",\n",
    "            \"Vdd\":\"vdd\",\n",
    "            \"break+ing\":\"breaking\",\n",
    "            \"marker+s\":\"markers\",\n",
    "            \"do+ed\":\"did\",\n",
    "            \"horse+s\":\"horses\",\n",
    "            \"Vdg\":\"vdg\",\n",
    "            \"unicorn+s\":\"unicorns\",\n",
    "            \"other+s\":\"others\"\n",
    "        }\n",
    "\n",
    "        for old, new in replacements.items():\n",
    "            tree = tree.replace(old, new)\n",
    "\n",
    "        # Transform cases\n",
    "        tree = tree.replace(\"'s+\", \"'s\")\n",
    "        tree = tree.replace(\"\\\\(\", \"-LRB-\")\n",
    "        tree = tree.replace(\"\\\\)\", \"-RRB-\")\n",
    "\n",
    "        adjusted_trees.append(tree)\n",
    "    return adjusted_trees\n",
    "\n",
    "adjusted_gold_trees = adjust_gold_trees(gold_trees)\n",
    "adjusted_charniak_trees = [tree[4:-1].strip() for tree in charniak_trees]\n",
    "\n",
    "# write to file\n",
    "def write_to_file(list_of_trees, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for tree in list_of_trees:\n",
    "            file.write(tree + '\\n')\n",
    "\n",
    "# Write adjusted trees to files\n",
    "write_to_file(adjusted_charniak_trees, 'charniak_trees.txt')\n",
    "write_to_file(berkeley_trees, 'berkeley_trees.txt')\n",
    "write_to_file(adjusted_gold_trees, 'gold_trees.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evalb (https://nlp.cs.nyu.edu/evalb/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sent.                        Matched  Bracket   Cross        Correct Tag\n",
      " ID  Len.  Stat. Recal  Prec.  Bracket gold test Bracket Words  Tags Accracy\n",
      "============================================================================\n",
      "   1   10    0   62.50  83.33     5      8    6      1      9     8    88.89\n",
      "   2   10    0   55.56  83.33     5      9    6      0      9     9   100.00\n",
      "   3   11    0   60.00  75.00     6     10    8      0     10     8    80.00\n",
      "   4   10    0   45.45  83.33     5     11    6      0      9     8    88.89\n",
      "   5   15    0   66.67  62.50    10     15   16      0     14    13    92.86\n",
      "   6   16    0   63.16  75.00    12     19   16      1     15    14    93.33\n",
      "   7   21    0   57.69  68.18    15     26   22      1     18    13    72.22\n",
      "   8   27    1    0.00   0.00     0      0    0      0      0     0     0.00\n",
      "   9   53    1    0.00   0.00     0      0    0      0      0     0     0.00\n",
      "  10   18    1    0.00   0.00     0      0    0      0      0     0     0.00\n",
      "  11   40    0   33.33  37.50    12     36   32     12     38    29    76.32\n",
      "============================================================================\n",
      "                 52.24  62.50     70   134   112     15    122   102    83.61\n",
      "=== Summary ===\n",
      "\n",
      "-- All --\n",
      "Number of sentence        =     11\n",
      "Number of Error sentence  =      3\n",
      "Number of Skip  sentence  =      0\n",
      "Number of Valid sentence  =      8\n",
      "Bracketing Recall         =  52.24\n",
      "Bracketing Precision      =  62.50\n",
      "Bracketing FMeasure       =  56.91\n",
      "Complete match            =   0.00\n",
      "Average crossing          =   1.88\n",
      "No crossing               =  50.00\n",
      "2 or less crossing        =  87.50\n",
      "Tagging accuracy          =  83.61\n",
      "\n",
      "-- len<=40 --\n",
      "Number of sentence        =     10\n",
      "Number of Error sentence  =      2\n",
      "Number of Skip  sentence  =      0\n",
      "Number of Valid sentence  =      8\n",
      "Bracketing Recall         =  52.24\n",
      "Bracketing Precision      =  62.50\n",
      "Bracketing FMeasure       =  56.91\n",
      "Complete match            =   0.00\n",
      "Average crossing          =   1.88\n",
      "No crossing               =  50.00\n",
      "2 or less crossing        =  87.50\n",
      "Tagging accuracy          =  83.61\n",
      "\n",
      "  Sent.                        Matched  Bracket   Cross        Correct Tag\n",
      " ID  Len.  Stat. Recal  Prec.  Bracket gold test Bracket Words  Tags Accracy\n",
      "============================================================================\n",
      "   1   10    0   62.50  83.33     5      8    6      1      9     9   100.00\n",
      "   2   10    0   55.56  83.33     5      9    6      0      9     9   100.00\n",
      "   3   11    0   60.00  66.67     6     10    9      1     10     9    90.00\n",
      "   4   10    0   45.45  83.33     5     11    6      0      9     7    77.78\n",
      "   5   15    0   66.67  62.50    10     15   16      0     14    13    92.86\n",
      "   6   16    0   63.16  75.00    12     19   16      1     15    13    86.67\n",
      "   7   21    0   50.00  76.47    13     26   17      0     18    17    94.44\n",
      "   8   27    0   61.76  87.50    21     34   24      0     24    23    95.83\n",
      "   9   53    0   35.71  60.61    20     56   33      6     43    38    88.37\n",
      "  10   18    0   38.89  70.00     7     18   10      2     16    12    75.00\n",
      "  11   40    0   25.00  28.12     9     36   32     15     38    32    84.21\n",
      "============================================================================\n",
      "                 46.69  64.57    113   242   175     26    205   182    88.78\n",
      "=== Summary ===\n",
      "\n",
      "-- All --\n",
      "Number of sentence        =     11\n",
      "Number of Error sentence  =      0\n",
      "Number of Skip  sentence  =      0\n",
      "Number of Valid sentence  =     11\n",
      "Bracketing Recall         =  46.69\n",
      "Bracketing Precision      =  64.57\n",
      "Bracketing FMeasure       =  54.20\n",
      "Complete match            =   0.00\n",
      "Average crossing          =   2.36\n",
      "No crossing               =  45.45\n",
      "2 or less crossing        =  81.82\n",
      "Tagging accuracy          =  88.78\n",
      "\n",
      "-- len<=40 --\n",
      "Number of sentence        =     10\n",
      "Number of Error sentence  =      0\n",
      "Number of Skip  sentence  =      0\n",
      "Number of Valid sentence  =     10\n",
      "Bracketing Recall         =  50.00\n",
      "Bracketing Precision      =  65.49\n",
      "Bracketing FMeasure       =  56.71\n",
      "Complete match            =   0.00\n",
      "Average crossing          =   2.00\n",
      "No crossing               =  50.00\n",
      "2 or less crossing        =  90.00\n",
      "Tagging accuracy          =  88.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "8 : Length unmatch (24|23)\n",
      "9 : Length unmatch (43|44)\n",
      "10 : Length unmatch (16|15)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_evalb(evalb_path, gold_file, test_file, result_file):\n",
    "\n",
    "    # Command to run EVALB\n",
    "    command = [evalb_path, '-p', 'EVALB/COLLINS.prm', gold_file, test_file]\n",
    "\n",
    "    # Run EVALB and write output to result_file\n",
    "    with open(result_file, 'w') as output_file:\n",
    "        subprocess.run(command, stdout=output_file, text=True)\n",
    "\n",
    "\n",
    "for parser_to_eval in [\"charniak\", \"berkeley\"]:\n",
    "    test_file = f\"{parser_to_eval}_trees.txt\"\n",
    "    evalb_path = 'EVALB/evalb'  \n",
    "    result_file_path = f'{test_file.split(\"_\")[0]}_evalb.txt'\n",
    "\n",
    "    # Running EVALB\n",
    "    run_evalb(evalb_path, \"gold_trees.txt\", test_file, result_file_path)\n",
    "\n",
    "    # Optionally, read and print the results\n",
    "    with open(result_file_path, 'r') as file:\n",
    "        results = file.read()\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(charniak_trees[0])\n",
    "print(berkeley_trees[0])\n",
    "print(gold_trees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Charniak:\", adjusted_charniak_trees[8])\n",
    "# print(berkeley_trees[6])\n",
    "print(\"Gold:\", adjusted_gold_trees[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
